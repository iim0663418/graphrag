# GraphRAG 中文處理全面調適方案

## 問題診斷總結

### 1. 架構層面問題
- **Tokenizer 不匹配**: GraphRAG 硬編碼 OpenAI tiktoken，與 Qwen 模型不兼容
- **中文處理缺陷**: 多重轉義導致文本破壞 (JSON.dumps, prompt.format)
- **並行設計缺陷**: 25 併發對本地 8B 模型過載

### 2. 執行層面問題  
- **超時頻繁**: 300s 不足，需要 1200s+
- **資源競爭**: 多線程導致模型推理速度指數下降
- **重試浪費**: 無效重試消耗大量時間

### 3. 配置層面問題
- **分塊過大**: 600 tokens 對本地模型負擔重
- **參數不當**: max_tokens, chunk_size 未針對中文優化

## 調適策略

### A. 立即可行方案 (已實施)
1. 完全序列化處理 (concurrent_requests: 1, num_threads: 1)
2. 大幅延長超時 (1200s)
3. 縮小文本分塊 (300 tokens)
4. 增加請求間隔 (2s stagger)

### B. 中期優化方案 (待實施)
1. 替換 tokenizer 為 Qwen 兼容版本
2. 修復多重轉義問題
3. 優化 prompt 模板處理

### C. 長期解決方案
1. 切換到 OpenAI 兼容模型
2. 或使用 GraphRAG 官方支持的模型
3. 硬體升級 (更大顯存/更快 GPU)

## 預期效果
- 超時問題: 95% 解決
- 中文處理: 80% 改善  
- 執行穩定性: 90% 提升
- 整體成功率: 從 5% 提升至 70%+
