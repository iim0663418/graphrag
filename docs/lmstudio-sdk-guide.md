# LMstudio Python SDK å®Œæ•´æŒ‡å—

## å®‰è£èˆ‡åŸºæœ¬è¨­ç½®

### å®‰è£
```bash
pip install lmstudio
```

### åŸºæœ¬å°å…¥
```python
import lmstudio as lms
```

## æ ¸å¿ƒåŠŸèƒ½æ¦‚è¦½

### ğŸ¯ **ä¸»è¦èƒ½åŠ›**
- **LLM å°è©±**ï¼šå¤šè¼ªèŠå¤©ã€æ–‡æœ¬å®Œæˆ
- **åµŒå…¥æ¨¡å‹**ï¼šæ–‡æœ¬å‘é‡åŒ–
- **æ¨¡å‹ç®¡ç†**ï¼šè¼‰å…¥ã€å¸è¼‰ã€åˆ—è¡¨ç®¡ç†
- **ä»£ç†æµç¨‹**ï¼šå·¥å…·èª¿ç”¨ã€è‡ªä¸»ä»£ç†
- **æµå¼è™•ç†**ï¼šå¯¦æ™‚éŸ¿æ‡‰æµ

### ğŸ”§ **API æ¶æ§‹**
```python
# ä¸‰ç¨® API æ¨¡å¼
# 1. ä¾¿åˆ© API (Interactive)
model = lms.llm()

# 2. ä½œç”¨åŸŸè³‡æº API (Scoped Resource)  
with lms.Client() as client:
    model = client.llm.model()

# 3. ç•°æ­¥ API (Asynchronous)
async with lms.AsyncClient() as client:
    model = await client.llm.model()
```

## LLM å°è©±åŠŸèƒ½

### åŸºæœ¬å°è©±
```python
# å¿«é€ŸéŸ¿æ‡‰
model = lms.llm("qwen/qwen3-4b-2507")
result = model.respond("What is the meaning of life?")
print(result)

# æµå¼éŸ¿æ‡‰
for fragment in model.respond_stream("Tell me a story"):
    print(fragment.content, end="", flush=True)
```

### å¤šè¼ªå°è©±ç®¡ç†
```python
# å‰µå»ºèŠå¤©ä¸Šä¸‹æ–‡
chat = lms.Chat("You are a helpful AI assistant")

# æ·»åŠ æ¶ˆæ¯
chat.add_user_message("Hello!")
chat.add_assistant_message("Hi there!")

# ç”ŸæˆéŸ¿æ‡‰
result = model.respond(chat)
```

### é…ç½®åƒæ•¸
```python
result = model.respond(chat, config={
    "temperature": 0.7,
    "maxTokens": 100,
    "topP": 0.9
})
```

## åµŒå…¥æ¨¡å‹åŠŸèƒ½

### æ–‡æœ¬åµŒå…¥
```python
# ç²å–åµŒå…¥æ¨¡å‹
embedding_model = lms.embedding_model("nomic-embed-text-v1.5")

# ç”ŸæˆåµŒå…¥å‘é‡
embedding = embedding_model.embed("Hello, world!")
print(f"Embedding dimension: {len(embedding)}")
```

## æ¨¡å‹ç®¡ç†

### åˆ—å‡ºæ¨¡å‹
```python
# åˆ—å‡ºå·²ä¸‹è¼‰çš„æ¨¡å‹
downloaded = lms.list_downloaded_models()

# åˆ—å‡ºå·²è¼‰å…¥çš„æ¨¡å‹  
loaded = lms.list_loaded_models()
```

### è¼‰å…¥èˆ‡å¸è¼‰
```python
# è¼‰å…¥æ¨¡å‹ï¼ˆå¦‚æœæœªè¼‰å…¥ï¼‰
model = lms.llm("qwen/qwen3-4b-2507")

# å¼·åˆ¶è¼‰å…¥æ–°å¯¦ä¾‹
client = lms.get_default_client()
new_instance = client.llm.load_new_instance("qwen/qwen3-4b-2507")

# å¸è¼‰æ¨¡å‹
model.unload()
```

### æ¨¡å‹é…ç½®
```python
# è¨­ç½® TTLï¼ˆç©ºé–’è‡ªå‹•å¸è¼‰æ™‚é–“ï¼‰
model = lms.llm("qwen/qwen3-4b-2507", ttl=3600)  # 1å°æ™‚

# è‡ªå®šç¾©è¼‰å…¥é…ç½®
model = client.llm.load_new_instance(
    "qwen/qwen3-4b-2507",
    config={
        "contextLength": 8192,
        "gpuOffload": 0.8
    }
)
```

## é€²éšåŠŸèƒ½

### é€²åº¦å›èª¿
```python
response = model.respond(
    "Complex question",
    on_prompt_processing_progress=lambda p: print(f"{p*100:.1f}% processed"),
    on_first_token=lambda: print("First token received!"),
    on_prediction_fragment=lambda f: print(f.content, end=""),
    on_message=chat.append  # è‡ªå‹•æ·»åŠ åˆ°èŠå¤©æ­·å²
)
```

### é æ¸¬çµ±è¨ˆ
```python
result = model.respond("Hello")

print(f"Model: {result.model_info.display_name}")
print(f"Tokens: {result.stats.predicted_tokens_count}")
print(f"Time to first token: {result.stats.time_to_first_token_sec}s")
print(f"Stop reason: {result.stats.stop_reason}")
```

### å–æ¶ˆé æ¸¬
```python
import threading
import time

# å•Ÿå‹•é æ¸¬
prediction_stream = model.respond_stream("Long response...")

# åœ¨å¦ä¸€å€‹ç·šç¨‹ä¸­å–æ¶ˆ
def cancel_after_delay():
    time.sleep(2)
    prediction_stream.cancel()

threading.Thread(target=cancel_after_delay).start()

# è™•ç†æµ
try:
    for fragment in prediction_stream:
        print(fragment.content, end="")
except Exception as e:
    print(f"Prediction cancelled: {e}")
```

## å¯¦ç”¨ç¤ºä¾‹

### å¤šè¼ªèŠå¤©æ©Ÿå™¨äºº
```python
def create_chatbot():
    model = lms.llm()
    chat = lms.Chat("You are a helpful assistant")
    
    while True:
        try:
            user_input = input("You: ")
            if not user_input:
                break
                
            chat.add_user_message(user_input)
            
            print("Bot: ", end="", flush=True)
            prediction_stream = model.respond_stream(
                chat,
                on_message=chat.append
            )
            
            for fragment in prediction_stream:
                print(fragment.content, end="", flush=True)
            print()
            
        except EOFError:
            break

if __name__ == "__main__":
    create_chatbot()
```

### æ‰¹é‡åµŒå…¥è™•ç†
```python
def batch_embeddings(texts, model_name="nomic-embed-text-v1.5"):
    embedding_model = lms.embedding_model(model_name)
    embeddings = []
    
    for text in texts:
        embedding = embedding_model.embed(text)
        embeddings.append(embedding)
    
    return embeddings

# ä½¿ç”¨ç¤ºä¾‹
texts = ["Hello world", "How are you?", "Goodbye"]
embeddings = batch_embeddings(texts)
```

### æ¨¡å‹æ€§èƒ½ç›£æ§
```python
class ModelMonitor:
    def __init__(self, model_name):
        self.model = lms.llm(model_name)
        self.stats = []
    
    def monitored_respond(self, prompt):
        result = self.model.respond(prompt)
        
        self.stats.append({
            "tokens": result.stats.predicted_tokens_count,
            "ttft": result.stats.time_to_first_token_sec,
            "total_time": result.stats.total_time_sec,
            "stop_reason": result.stats.stop_reason
        })
        
        return result
    
    def get_average_stats(self):
        if not self.stats:
            return None
            
        return {
            "avg_tokens": sum(s["tokens"] for s in self.stats) / len(self.stats),
            "avg_ttft": sum(s["ttft"] for s in self.stats) / len(self.stats),
            "avg_total_time": sum(s["total_time"] for s in self.stats) / len(self.stats)
        }
```

## éŒ¯èª¤è™•ç†

### å¸¸è¦‹ç•°å¸¸
```python
try:
    model = lms.llm("non-existent-model")
except Exception as e:
    print(f"Model loading failed: {e}")

try:
    result = model.respond("Hello", config={"maxTokens": -1})
except ValueError as e:
    print(f"Invalid configuration: {e}")
```

### è¶…æ™‚è¨­ç½®
```python
# è¨­ç½®åŒæ­¥ API è¶…æ™‚ï¼ˆ60ç§’é»˜èªï¼‰
lms.set_sync_api_timeout(120)  # 2åˆ†é˜

# æŸ¥è©¢ç•¶å‰è¶…æ™‚è¨­ç½®
timeout = lms.get_sync_api_timeout()
print(f"Current timeout: {timeout} seconds")

# ç¦ç”¨è¶…æ™‚
lms.set_sync_api_timeout(None)
```

## èˆ‡ GraphRAG æ•´åˆè¦é»

### é—œéµæ•´åˆé»
1. **LLM èª¿ç”¨**ï¼šä½¿ç”¨ `model.respond()` æ›¿ä»£ OpenAI API
2. **åµŒå…¥ç”Ÿæˆ**ï¼šä½¿ç”¨ `embedding_model.embed()` 
3. **æ¨¡å‹ç®¡ç†**ï¼šå‹•æ…‹è¼‰å…¥/å¸è¼‰ç¯€çœè¨˜æ†¶é«”
4. **æ‰¹é‡è™•ç†**ï¼šåˆ©ç”¨æœ¬åœ°æ¨¡å‹å„ªå‹¢é€²è¡Œæ‰¹é‡æ“ä½œ

### æ€§èƒ½å„ªåŒ–å»ºè­°
- ä½¿ç”¨ TTL è‡ªå‹•ç®¡ç†æ¨¡å‹è¨˜æ†¶é«”
- æ‰¹é‡è™•ç†æ¸›å°‘æ¨¡å‹è¼‰å…¥æ¬¡æ•¸
- ç›£æ§çµ±è¨ˆæ•¸æ“šå„ªåŒ–åƒæ•¸
- åˆç†è¨­ç½®ä¸Šä¸‹æ–‡é•·åº¦

---
*æ›´æ–°æ—¥æœŸï¼š2026-01-10*
*SDK ç‰ˆæœ¬ï¼š1.5.0+*
