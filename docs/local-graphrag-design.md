# æœ¬åœ°åŒ– GraphRAG æ–¹æ¡ˆè¨­è¨ˆ

## å•é¡Œåˆ†æ

### Microsoft GraphRAG æ ¸å¿ƒå¼±é»
1. **ç´¢å¼•æˆæœ¬é«˜**ï¼šå¤§é‡ LLM API èª¿ç”¨ï¼ˆå¯¦é«”æå–ã€é—œä¿‚å»ºæ§‹ã€ç¤¾ç¾¤æ‘˜è¦ï¼‰
2. **æ¨¡å‹é¸æ“‡å—é™**ï¼šä¸»è¦ä¾è³´ OpenAI/Azure OpenAI

## æœ¬åœ°åŒ–è§£æ±ºæ–¹æ¡ˆ

### æ¶æ§‹è¨­è¨ˆ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LMstudio      â”‚    â”‚  Local GraphRAG â”‚    â”‚   Web UI        â”‚
â”‚   - LLM Server  â”‚â—„â”€â”€â–ºâ”‚   - å¯¦é«”æå–     â”‚â—„â”€â”€â–ºâ”‚   - æ–‡æª”ç®¡ç†     â”‚
â”‚   - åµŒå…¥æ¨¡å‹     â”‚    â”‚   - é—œä¿‚å»ºæ§‹     â”‚    â”‚   - æŸ¥è©¢ä»‹é¢     â”‚
â”‚   - API ç›¸å®¹     â”‚    â”‚   - åœ–æ¨ç†       â”‚    â”‚   - çµæœè¦–è¦ºåŒ–   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒçµ„ä»¶

#### 1. LMstudio æ•´åˆå±¤
```python
# lmstudio_client.py
class LMStudioClient:
    def __init__(self, base_url="http://localhost:1234"):
        self.base_url = base_url
        self.client = OpenAI(base_url=base_url, api_key="lm-studio")
    
    def chat_completion(self, messages, model="local-model"):
        return self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.1
        )
    
    def embedding(self, text, model="embedding-model"):
        return self.client.embeddings.create(
            model=model,
            input=text
        )
```

#### 2. æˆæœ¬å„ªåŒ–ç­–ç•¥
```python
# cost_optimizer.py
class LocalGraphRAGOptimizer:
    def __init__(self):
        self.entity_cache = {}
        self.relationship_cache = {}
    
    def batch_entity_extraction(self, texts, batch_size=10):
        """æ‰¹æ¬¡è™•ç†é™ä½ LLM èª¿ç”¨æ¬¡æ•¸"""
        batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]
        results = []
        
        for batch in batches:
            combined_text = "\n---\n".join(batch)
            entities = self.extract_entities_batch(combined_text)
            results.extend(entities)
        
        return results
    
    def cached_relationship_extraction(self, entity_pairs):
        """é—œä¿‚æå–å¿«å–æ©Ÿåˆ¶"""
        cache_key = hash(tuple(sorted(entity_pairs)))
        if cache_key in self.relationship_cache:
            return self.relationship_cache[cache_key]
        
        relationships = self.extract_relationships(entity_pairs)
        self.relationship_cache[cache_key] = relationships
        return relationships
```

#### 3. æ¨¡å‹é…ç½®ç®¡ç†
```python
# model_config.py
LOCAL_MODELS = {
    "llm": {
        "model_name": "llama-3.1-8b-instruct",
        "context_length": 8192,
        "temperature": 0.1
    },
    "embedding": {
        "model_name": "bge-large-zh-v1.5",
        "dimension": 1024
    }
}

class ModelManager:
    def __init__(self, lmstudio_client):
        self.client = lmstudio_client
        self.models = LOCAL_MODELS
    
    def get_available_models(self):
        """ç²å– LMstudio å¯ç”¨æ¨¡å‹"""
        return self.client.models.list()
    
    def switch_model(self, model_type, model_name):
        """å‹•æ…‹åˆ‡æ›æ¨¡å‹"""
        self.models[model_type]["model_name"] = model_name
```

### å¯¦ä½œæ­¥é©Ÿ

#### Phase 1: åŸºç¤æ•´åˆï¼ˆ1é€±ï¼‰
```bash
# 1. å®‰è£ä¾è³´
pip install graphrag openai sentence-transformers

# 2. é…ç½® LMstudio
# - ä¸‹è¼‰ Llama 3.1 8B Instruct
# - ä¸‹è¼‰ BGE-Large-ZH åµŒå…¥æ¨¡å‹
# - å•Ÿå‹• API æœå‹™ï¼ˆç«¯å£ 1234ï¼‰

# 3. ä¿®æ”¹ GraphRAG é…ç½®
```

```yaml
# settings.yaml
models:
  - model: local-llm
    type: chat
    api_base: http://localhost:1234/v1
    api_key: lm-studio
    
  - model: local-embedding  
    type: embedding
    api_base: http://localhost:1234/v1
    api_key: lm-studio
```

#### Phase 2: æˆæœ¬å„ªåŒ–ï¼ˆ1é€±ï¼‰
```python
# local_graphrag.py
class LocalGraphRAG:
    def __init__(self):
        self.lm_client = LMStudioClient()
        self.optimizer = LocalGraphRAGOptimizer()
        
    def optimized_indexing(self, documents):
        """å„ªåŒ–çš„ç´¢å¼•æµç¨‹"""
        # 1. æ‰¹æ¬¡å¯¦é«”æå–
        entities = self.optimizer.batch_entity_extraction(documents)
        
        # 2. å¿«å–é—œä¿‚æå–  
        relationships = self.optimizer.cached_relationship_extraction(entities)
        
        # 3. å¢é‡ç¤¾ç¾¤æª¢æ¸¬
        communities = self.incremental_community_detection(relationships)
        
        return {
            "entities": entities,
            "relationships": relationships, 
            "communities": communities
        }
```

#### Phase 3: UI æ•´åˆï¼ˆ1é€±ï¼‰
```python
# app.py - Gradio UI
import gradio as gr
from local_graphrag import LocalGraphRAG

def create_ui():
    graphrag = LocalGraphRAG()
    
    with gr.Blocks(title="æœ¬åœ° GraphRAG") as app:
        gr.Markdown("# ğŸ  æœ¬åœ°åŒ– GraphRAG ç³»çµ±")
        
        with gr.Tab("æ–‡æª”ç´¢å¼•"):
            file_input = gr.File(label="ä¸Šå‚³æ–‡æª”", file_count="multiple")
            index_btn = gr.Button("é–‹å§‹ç´¢å¼•", variant="primary")
            index_output = gr.Textbox(label="ç´¢å¼•çµæœ")
            
        with gr.Tab("æ™ºèƒ½æŸ¥è©¢"):
            query_input = gr.Textbox(label="è¼¸å…¥å•é¡Œ")
            search_btn = gr.Button("æœå°‹", variant="primary") 
            result_output = gr.Textbox(label="æŸ¥è©¢çµæœ")
            
        with gr.Tab("æ¨¡å‹ç®¡ç†"):
            model_dropdown = gr.Dropdown(label="é¸æ“‡æ¨¡å‹")
            model_info = gr.Textbox(label="æ¨¡å‹è³‡è¨Š")
    
    return app

if __name__ == "__main__":
    app = create_ui()
    app.launch(server_name="0.0.0.0", server_port=7860)
```

### æ•ˆèƒ½å„ªåŒ–ç­–ç•¥

#### 1. è¨˜æ†¶é«”ç®¡ç†
```python
# memory_optimizer.py
class MemoryOptimizer:
    def __init__(self, max_cache_size=1000):
        self.max_cache_size = max_cache_size
        self.entity_cache = {}
        
    def smart_caching(self, key, value):
        """æ™ºèƒ½å¿«å–ç®¡ç†"""
        if len(self.entity_cache) >= self.max_cache_size:
            # LRU æ·˜æ±°ç­–ç•¥
            oldest_key = next(iter(self.entity_cache))
            del self.entity_cache[oldest_key]
        
        self.entity_cache[key] = value
```

#### 2. ä¸¦è¡Œè™•ç†
```python
# parallel_processor.py
import asyncio
from concurrent.futures import ThreadPoolExecutor

class ParallelProcessor:
    def __init__(self, max_workers=4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def parallel_entity_extraction(self, text_chunks):
        """ä¸¦è¡Œå¯¦é«”æå–"""
        tasks = []
        for chunk in text_chunks:
            task = asyncio.create_task(self.extract_entities_async(chunk))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results
```

### éƒ¨ç½²é…ç½®

#### Docker éƒ¨ç½²
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 7860

CMD ["python", "app.py"]
```

#### è³‡æºéœ€æ±‚
```yaml
# æœ€ä½é…ç½®
CPU: 8 æ ¸å¿ƒ
RAM: 16GB
GPU: RTX 4060 8GB (å¯é¸)
å­˜å„²: 50GB SSD

# æ¨è–¦é…ç½®  
CPU: 16 æ ¸å¿ƒ
RAM: 32GB
GPU: RTX 4090 24GB
å­˜å„²: 100GB NVMe SSD
```

## é æœŸæ•ˆç›Š

### æˆæœ¬ç¯€çœ
- **LLM èª¿ç”¨æˆæœ¬**: 100% ç¯€çœï¼ˆç´”æœ¬åœ°ï¼‰
- **ç´¢å¼•æ™‚é–“**: æ¸›å°‘ 60%ï¼ˆæ‰¹æ¬¡è™•ç† + å¿«å–ï¼‰
- **ç¡¬é«”æˆæœ¬**: ä¸€æ¬¡æ€§æŠ•è³‡ï¼Œé•·æœŸä½¿ç”¨

### æŠ€è¡“å„ªå‹¢
- âœ… **å®Œå…¨é›¢ç·š**: ç„¡éœ€ç¶²è·¯é€£æ¥
- âœ… **è³‡æ–™éš±ç§**: è³‡æ–™ä¸é›¢é–‹æœ¬åœ°
- âœ… **æ¨¡å‹è‡ªç”±**: æ”¯æ´ä»»ä½• LMstudio ç›¸å®¹æ¨¡å‹
- âœ… **æˆæœ¬å¯æ§**: ç„¡ API èª¿ç”¨è²»ç”¨

---
*è¨­è¨ˆæ—¥æœŸï¼š2026-01-10*
*é è¨ˆé–‹ç™¼é€±æœŸï¼š3é€±*
